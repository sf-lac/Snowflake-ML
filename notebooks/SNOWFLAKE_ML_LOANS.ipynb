{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "452d8b95-12a9-4560-b083-9c0bf5a43f76",
   "metadata": {
    "collapsed": false,
    "name": "title"
   },
   "source": [
    "# End-to-end Snowflake ML workflow for loan lending prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe93a23-e369-406d-b8ec-e074d6a6cecd",
   "metadata": {
    "collapsed": false,
    "name": "snowflake_ml"
   },
   "source": [
    "# __Featured Snowflake ML Capabilities__ \n",
    "\n",
    "- __`Distributed ML Processing`__: `Snowflake Container Runtime for ML` with the `Ray` framework to process and train models efficiently across multiple compute nodes.\n",
    "- __`Data Ingestion & Persistance`__: `Snowflake ML Data Loading APIs` including\n",
    "    - `DataSource APIs` for ingesting staged data (CSV, Parquet, unstructured),\n",
    "    - `DataConnector` for loading structured data from Snowflake tables and Snowflake datasets in ML workflows,\n",
    "    - `DataSink` for writing transformed data, ML predictions and inference results back to Snowflake tables.\n",
    "- __`Feature Engineering`__: centralized `Feature Store` with registered entities and feature views; reusable `Snowflake Dataset` generation directly from the feature store for training and inference consistency.\n",
    "- __`Model Development & Experimentation`__: `Snowflake ML APIs` for training and tuning models, including hyperparameter optimization, experiment tracking, and artifacts logging via Snowflake's native `Experiment Manager`.\n",
    "- __`Model Registry & Lifecycle Management`__: Model registration in the `Snowflake Model Registry` with full versioning, lifecycle control, and support for batch and distributed inference using Python, SQL, or REST API endpoints.\n",
    "- __`Model Explainability`__: Feature impact evaluation using `Model Registry explainability` powered by `Shapley values`, enabling transparent assessment of individual feature contributions to loan lending prediction.\n",
    "- __`ML Observability`__: Monitor deployed models for performance, data drift, and prediction volume using stored inference data and Snowflake's ML observability capabilities. \n",
    "- __`Model Serving`__: Deploy and serve models in production using Snowpark Container Services (SPCS) for scalable, low-latency inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfed4ef-7cb4-4ad2-b090-29379fe71c69",
   "metadata": {
    "collapsed": false,
    "name": "setup_worksheet"
   },
   "source": [
    "```sql\n",
    "USE ROLE ACCOUNTADMIN;\n",
    "SET USERNAME = (SELECT CURRENT_USER());\n",
    "CREATE OR REPLACE ROLE MLOPS_ROLE;\n",
    "\n",
    "-- Grant necessary permissions to create databases, compute pools, and service endpoints to new role\n",
    "GRANT CREATE DATABASE on ACCOUNT to ROLE MLOPS_ROLE; \n",
    "GRANT CREATE COMPUTE POOL on ACCOUNT to ROLE MLOPS_ROLE;\n",
    "GRANT CREATE WAREHOUSE ON ACCOUNT to ROLE MLOPS_ROLE;\n",
    "GRANT BIND SERVICE ENDPOINT on ACCOUNT to ROLE MLOPS_ROLE;\n",
    "\n",
    "-- grant new role to user and switch to that role\n",
    "GRANT ROLE MLOPS_ROLE to USER identifier($USERNAME);\n",
    "USE ROLE MLOPS_ROLE;\n",
    "\n",
    "-- Create warehouse\n",
    "CREATE OR REPLACE WAREHOUSE MLOPS_WH WITH WAREHOUSE_SIZE='SMALL';\n",
    "\n",
    "-- Create Database \n",
    "CREATE OR REPLACE DATABASE MLOPS_DB;\n",
    "\n",
    "-- Create Schema\n",
    "CREATE OR REPLACE SCHEMA MLOPS_SCHEMA;\n",
    "\n",
    "-- Create compute pool for Snowpark Container Services\n",
    "CREATE COMPUTE POOL IF NOT EXISTS MLOPS_COMPUTE_POOL \n",
    " MIN_NODES = 1\n",
    " MAX_NODES = 1\n",
    " INSTANCE_FAMILY = CPU_X64_M;\n",
    "\n",
    "-- Describe compute pool\n",
    "DESCRIBE COMPUTE POOL MLOPS_COMPUTE_POOL;\n",
    "\n",
    "-- Create notebook \n",
    "CREATE OR REPLACE NOTEBOOK MLOPS_DB.MLOPS_SCHEMA.MLOPS_NB \n",
    "MAIN_FILE = 'Snowflake_ML_Loans.ipynb' \n",
    "QUERY_WAREHOUSE = MLOPS_WH\n",
    "RUNTIME_NAME = 'SYSTEM$BASIC_RUNTIME' --uses Snowpark Container Services (SPCS) infrastructure running on a compute pool\n",
    "COMPUTE_POOL = 'MLOPS_COMPUTE_POOL'\n",
    "IDLE_AUTO_SHUTDOWN_TIME_SECONDS = 3600;\n",
    "\n",
    "CREATE STAGE IF NOT EXISTS MLOPS_DB.MLOPS_SCHEMA.MLOPS_STAGE\n",
    "  DIRECTORY = (ENABLE = TRUE)\n",
    "  FILE_FORMAT = (TYPE = 'CSV');  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "collapsed": false,
    "name": "configure_environment"
   },
   "source": [
    "### Initialize environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d6ee27-b456-45fc-b9e2-4d2247b7e445",
   "metadata": {
    "language": "python",
    "name": "check_packages"
   },
   "outputs": [],
   "source": [
    "!pip freeze | grep snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41afbe26-0784-424a-ae96-2f92e78accc5",
   "metadata": {
    "language": "python",
    "name": "install_shap"
   },
   "outputs": [],
   "source": [
    "# SHAP (SHapley Additive exPlanations) \n",
    "# a unified approach to explain the output of any machine learning model.\n",
    "\n",
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80fd38c-8f5b-418d-a6df-3d5d1111cbc6",
   "metadata": {
    "language": "python",
    "name": "filter_warnings"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\"   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16821394-080f-4f65-8811-afa609216c07",
   "metadata": {
    "language": "python",
    "name": "intitialize_variables"
   },
   "outputs": [],
   "source": [
    "VERSION_NUM = '1'\n",
    "DB = \"MLOPS_DB\" \n",
    "SCHEMA = \"MLOPS_SCHEMA\" \n",
    "COMPUTE_WAREHOUSE = \"MLOPS_WH\" \n",
    "ROLE = \"MLOPS_ROLE\"\n",
    "ORG_NAME = \"xxxxxxx\"\n",
    "ACCOUNT_NAME = \"xxxxxxxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ad2356-c363-465f-8167-7692426d7597",
   "metadata": {
    "language": "python",
    "name": "initialize_session"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4b4efc-450e-4ebc-9be8-f29dfcd07745",
   "metadata": {
    "collapsed": false,
    "name": "load_data_ray"
   },
   "source": [
    "# Optimized Data Loading with Snowflake ML DataSource & DataSink APIs \n",
    "\n",
    "- Raw loan application data is uploaded to a Snowflake internal stage using the Snowsight UI\n",
    "\n",
    "- CSV (*.csv) files are ingested from the stage using Snowflake ML DataSource APIs\n",
    "\n",
    "- Ray is used to parallelize data loading and preprocessing across multiple compute nodes, improving throughput and scalability\n",
    "\n",
    "- Ray datasets are written back to Snowflake using DataSink for downstream feature engineering and model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2e4834-959b-442e-be69-524b2b031a89",
   "metadata": {
    "language": "python",
    "name": "set_ray_context"
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "context = ray.data.DataContext.get_current()\n",
    "context.execution_options.verbose_progress = False\n",
    "context.enable_operator_progress_bars = False\n",
    "context.enable_progress_bars = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feed4736-e6d2-4368-ba64-a0f14d405aba",
   "metadata": {
    "language": "python",
    "name": "build_ray_dataset_from_source"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml.ray.datasource import SFStageCSVDataSource\n",
    "\n",
    "file_name = \"*.csv\"\n",
    "stage_name = \"@MLOPS_STAGE\"\n",
    "\n",
    "data_source = SFStageCSVDataSource(\n",
    "    stage_location=stage_name,\n",
    "    file_pattern=file_name\n",
    ")\n",
    "\n",
    "# Build Ray dataset from provided datasources\n",
    "ray_ds = ray.data.read_datasource(data_source)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d6f45a-debd-438f-ba1b-927a5c4db6ab",
   "metadata": {
    "collapsed": false,
    "name": "ray_ds_explained"
   },
   "source": [
    "The `ray.data.read_datasource()` function returns a `ray.data.Dataset`, a lazily evaluated, distributed dataset designed to scale beyond the memory of a single node. When ingesting data from Snowflake, the dataset is internally represented as __Apache Arrow tables__, enabling efficient, columnar data transfer. These Arrow tables are transparently converted to the required Python formats — such as __Pandas DataFrames__ or __NumPy arrays__ — during downstream processing or iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d8eb12-bd7b-4267-971c-d9fbcb467d68",
   "metadata": {
    "language": "python",
    "name": "ray_ds_show"
   },
   "outputs": [],
   "source": [
    "\n",
    "ray_ds.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eb00d8-6f89-434e-a426-9f52dc127833",
   "metadata": {
    "language": "python",
    "name": "create_ray_datasink"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml.ray.datasink import SnowflakeTableDatasink\n",
    "\n",
    "datasink = SnowflakeTableDatasink(\n",
    "    table_name=\"MLOPS_LOANS\",\n",
    "    database=\"MLOPS_DB\",\n",
    "    schema=\"MLOPS_SCHEMA\",\n",
    "    auto_create_table=True, \n",
    "    override=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283548aa-2133-45d9-a517-70aeeaf63160",
   "metadata": {
    "language": "python",
    "name": "write_datasink"
   },
   "outputs": [],
   "source": [
    "# Write to Snowflake distributedly\n",
    "ray_ds.write_datasink(datasink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7fe8ed-d801-488c-bf3c-cdb33b48a24e",
   "metadata": {
    "language": "sql",
    "name": "show_tables"
   },
   "outputs": [],
   "source": [
    "show tables;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c036c07-f56a-4316-887e-fa9ef8450c09",
   "metadata": {
    "language": "python",
    "name": "read_sinked_table"
   },
   "outputs": [],
   "source": [
    "sdf = session.table(\"MLOPS_LOANS\")\n",
    "sdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097f88ea-2e90-4271-81b1-71b81318e206",
   "metadata": {
    "collapsed": false,
    "name": "exploratory_data_analysis_title"
   },
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d86d1-b2ad-4630-96ba-347ed6917c61",
   "metadata": {
    "language": "python",
    "name": "snowpark_dataframe_columns"
   },
   "outputs": [],
   "source": [
    "sdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b07014-31bc-4127-9b7b-ef93672e579b",
   "metadata": {
    "language": "python",
    "name": "snowpark_dataframe_count"
   },
   "outputs": [],
   "source": [
    "sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fdbfac-0bc3-4b92-b484-b25c1a7b3ecf",
   "metadata": {
    "language": "python",
    "name": "import_snowpark_functions"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c3517c-54ae-4b9c-adc8-eadf71841ca5",
   "metadata": {
    "collapsed": false,
    "name": "balance_of_target_class"
   },
   "source": [
    "Inspect target label distribution (LOAN_RESPONSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f87713-a4bf-4f1d-b2d1-0826fa2ac163",
   "metadata": {
    "language": "python",
    "name": "target_class_balance"
   },
   "outputs": [],
   "source": [
    "sdf.select(col(\"LOAN_RESPONSE\")) \\\n",
    "   .group_by(col(\"LOAN_RESPONSE\")) \\\n",
    "   .count() \\\n",
    "   .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfc7d77-7829-4927-941d-0b89de55cf1f",
   "metadata": {
    "collapsed": false,
    "name": "feature_engineering"
   },
   "source": [
    "# Feature Engineering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b5aa8e-914f-4f9b-ad22-b23a7c588f7f",
   "metadata": {
    "language": "python",
    "name": "convert_ts_to_timestamp"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.functions import call_function\n",
    "\n",
    "# 1. Convert TS → timestamp (MM:SS.FF3)\n",
    "sdf = sdf.with_column(\n",
    "    \"TS_TIMESTAMP\",\n",
    "    to_timestamp(\n",
    "        concat(current_date(), lit(\" \"), col(\"TS\")),\n",
    "        \"YYYY-MM-DD MI:SS.FF3\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# 2. Max timestamp\n",
    "max_ts_sdf = sdf.select(max(col(\"TS_TIMESTAMP\")).alias(\"MAX_TS\"))\n",
    "\n",
    "# 3. Timedelta using SQL TIMESTAMPDIFF\n",
    "delta_sdf = max_ts_sdf.select(\n",
    "    call_function(\"timestampdiff\",\n",
    "                  lit(\"day\"),\n",
    "                  col(\"MAX_TS\"),\n",
    "                  current_timestamp()\n",
    "                 ).alias(\"DELTA\")\n",
    ")\n",
    "\n",
    "# Extract integer and subtract 1\n",
    "delta = delta_sdf.collect()[0][\"DELTA\"]\n",
    "time_shift = delta - 1\n",
    "\n",
    "# 4. Shift timestamps\n",
    "sdf = sdf.with_column(\n",
    "    \"TIMESTAMP\",\n",
    "    dateadd(\"day\", lit(time_shift), col(\"TS_TIMESTAMP\"))\n",
    ")\n",
    "\n",
    "# 5. Min/max shifted timestamps\n",
    "result = sdf.select(\n",
    "    min(col(\"TIMESTAMP\")).alias(\"MIN_SHIFTED\"),\n",
    "    max(col(\"TIMESTAMP\")).alias(\"MAX_SHIFTED\")\n",
    ")\n",
    "\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75880956-4c60-4465-8d3f-e34f6c59dbaf",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": [
    "sdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4bcfee-08b9-4932-a318-528b3bed8725",
   "metadata": {
    "language": "python",
    "name": "create_features_snowparkdataframe"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.types import DecimalType, IntegerType\n",
    "from snowflake.snowpark import Window\n",
    "\n",
    "sdf_features = sdf \\\n",
    "    .select(\"LOAN_ID\", \"TIMESTAMP\",           \n",
    "           \"LOAN_AMOUNT\", \"APPLICANT_INCOME\", \"COUNTY\") \\\n",
    "    .with_column(\"LOAN_AMOUNT\", col(\"LOAN_AMOUNT\")*1000) \\\n",
    "    .with_column(\"INCOME\", col(\"APPLICANT_INCOME\")*1000) \\\n",
    "    .with_column(\"INCOME_LOAN_RATIO\",(col(\"INCOME\") / col(\"LOAN_AMOUNT\")).cast(DecimalType(18,6)))\n",
    "    \n",
    "county_window_spec = Window.partition_by(\"COUNTY\")    \n",
    "sdf_features = sdf_features \\\n",
    "    .with_column(\"MEAN_COUNTY_INCOME\", avg(\"INCOME\").over(county_window_spec).cast(DecimalType(18,2))) \\\n",
    "    .with_column(\"HIGH_INCOME_FLAG\", (col(\"INCOME\")>col(\"MEAN_COUNTY_INCOME\")).cast(IntegerType())) \\\n",
    "    .with_column(\"AVG_DAILY_LOAN_AMOUNT\",\n",
    "    sql_expr(\"\"\"        \n",
    "            AVG(LOAN_AMOUNT) OVER (\n",
    "                PARTITION BY COUNTY \n",
    "                ORDER BY TIMESTAMP\n",
    "                RANGE BETWEEN INTERVAL '1 DAY' PRECEDING AND CURRENT ROW\n",
    "            )       \n",
    "    \"\"\").cast(DecimalType(18, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbb3d2c-16aa-4ba2-b946-29b46d6c3713",
   "metadata": {
    "language": "python",
    "name": "show_sdf_features"
   },
   "outputs": [],
   "source": [
    "sdf_features.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbef418-c0b0-4ed9-83e9-fcdca53104bb",
   "metadata": {
    "collapsed": false,
    "name": "feature_store_title"
   },
   "source": [
    "# Feature Store Overview\n",
    "\n",
    "A __`feature store`__ centralizes commonly used feature definitions and transformations, enabling feature reuse, reducing duplication, and improving ML team productivity. It ensures that features remain __consistent, up-to-date, and production-ready__, providing a single source of truth for both training and inference workflows.\n",
    "\n",
    "## Feature Store Concepts\n",
    "\n",
    "In Snowflake, a feature store is implemented as a __schema__. Can either create a dedicated schema for feature storage or reuse an existing one. All feature store objects are native Snowflake objects and are governed by Snowflake’s access control and security model.\n",
    "\n",
    "A Snowflake feature store consists of the following core components:\n",
    "\n",
    "__`Entities`__\n",
    "\n",
    "Entities represent real-world objects or concepts. They:\n",
    "\n",
    "- Organize feature views by subject area\n",
    "\n",
    "- Define the __join keys__ used to link features back to source data during training and inference\n",
    "\n",
    "__`Feature Views`__\n",
    "\n",
    "Feature views define and manage groups of related features. They:\n",
    "\n",
    "- Provide a structured, reusable definition of feature transformations\n",
    "\n",
    "- Refresh all contained features on a common schedule\n",
    "\n",
    "- Are backed by a __feature table__ (typically a Snowflake-managed __dynamic table__)\n",
    "\n",
    "- Support incremental updates to efficiently process new source data\n",
    "\n",
    "__`Datasets`__\n",
    "\n",
    "Datasets are curated collections of features assembled from one or more feature views for __model training or inference__, ensuring consistency across the ML lifecycle.\n",
    "\n",
    "## Feature Store Object Mapping\n",
    "\n",
    "| Feature Store Object | Snowflake Object |\n",
    "| :------- | :------- |\n",
    "| Feature store | Schema |\n",
    "| Feature view | Dynamic table or view |\n",
    "| Entity | Tag on dynamic table or view |\n",
    "| Feature | Column in a dynamic table or in a view |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c17968-3f68-428e-aabf-a32e6d1bfe3d",
   "metadata": {
    "language": "python",
    "name": "create_feature_store"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml.feature_store import FeatureStore, FeatureView, Entity, CreationMode\n",
    "\n",
    "fs = FeatureStore(\n",
    "    session = session,\n",
    "    database = DB,\n",
    "    name = SCHEMA, \n",
    "    default_warehouse=COMPUTE_WAREHOUSE,\n",
    "    creation_mode=CreationMode.CREATE_IF_NOT_EXIST # create new schema for feature store if not exists\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986a122a-40f8-4140-8797-5ef23b29763d",
   "metadata": {
    "language": "python",
    "name": "define_register_entity"
   },
   "outputs": [],
   "source": [
    "# retrieve an existing entity definition, if not define a new one and register it in the feature store\n",
    "\n",
    "try:\n",
    "    # retrieve existing entity\n",
    "    loan_entity = fs.get_entity(\"LOAN_ENTITY\")\n",
    "    print(\"Retrieved existing entity\")\n",
    "except:\n",
    "    # define new entity\n",
    "    loan_entity = Entity(\n",
    "        name = \"LOAN_ENTITY\",\n",
    "        join_keys = [\"LOAN_ID\"],\n",
    "        desc = \"Features defined on a per loan level\"\n",
    "    )\n",
    "    #register\n",
    "    fs.register_entity(loan_entity)\n",
    "    print(\"Registered new entity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01c4759-3d95-43de-a481-52e3798fe857",
   "metadata": {
    "language": "python",
    "name": "create_feature_dataframe"
   },
   "outputs": [],
   "source": [
    "# Snowpark DataFrame that contains the feature transformations used by feature view constructor\n",
    "feature_df = sdf_features.drop(\"APPLICANT_INCOME\", \"COUNTY\")\n",
    "feature_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5039cf7e-3983-4c07-a7b5-ad8d88b1b05c",
   "metadata": {
    "collapsed": false,
    "name": "feature_view_title"
   },
   "source": [
    "## Defining and Registering Feature Views\n",
    "\n",
    "A __`feature view`__ encapsulates a Python- or SQL-based transformation pipeline that converts raw source data into one or more related features. All features within a feature view are refreshed simultaneously from the underlying source data, ensuring consistency across training and inference.\n",
    "\n",
    "The __`snowflake.ml.feature_store.FeatureView`__ class provides the Python API for defining feature views. The constructor accepts a __Snowpark DataFrame__ containing the feature generation logic and must include:\n",
    "\n",
    "- The __join key columns__ defined by the associated entities\n",
    "\n",
    "- A __timestamp column__ when time-series features are present\n",
    "\n",
    "Once defined, a feature view can be registered in the feature store using the `register_feature_view` method, specifying a custom name, version, and refresh configuration. After registration:\n",
    "\n",
    "Feature extraction is __incrementally maintained__\n",
    "\n",
    "Features are __automatically refreshed__ according to the configured schedule\n",
    "\n",
    "Feature view definitions are __immutable after registration__, ensuring reproducible and consistent feature computation throughout the feature view’s lifecycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54c2c02-fb36-4bed-b188-e1387f22fa72",
   "metadata": {
    "language": "python",
    "name": "define_register_feature_view"
   },
   "outputs": [],
   "source": [
    "# define and register feature view\n",
    "loan_fv = FeatureView(\n",
    "    name = \"Loan_Feature_View\",\n",
    "    entities = [loan_entity],\n",
    "    feature_df = feature_df,        # features transformations Snowpark DataFrame\n",
    "    timestamp_col = \"TIMESTAMP\",\n",
    "    refresh_freq = \"1 day\"          # frequency of feature data refreshes\n",
    ")\n",
    "\n",
    "# add feature level descriptions to enhance feature view discoverability in Snowsight Universal Search\n",
    "loan_fv = loan_fv.attach_feature_desc(\n",
    "    {\n",
    "        \"LOAN_AMOUNT\": \"Loan amount in $USD\",\n",
    "        \"INCOME\": \"Household income in $USD\",\n",
    "        \"INCOME_LOAN_RATIO\": \"Ratio of LOAN_AMOUNT/INCOME\",\n",
    "        \"MEAN_COUNTY_INCOME\": \"Average household income aggregated at county level\",\n",
    "        \"HIGH_INCOME_FLAG\": \"Binary flag to indicate whether household income is higher than MEAN_COUNTY_INCOME\",\n",
    "        \"AVG_DAILY_LOAN_AMOUNT\": \"Rolling daily average of LOAN_AMOUNT\"\n",
    "    }\n",
    ")\n",
    "\n",
    "loan_fv = fs.register_feature_view(loan_fv, version=VERSION_NUM, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8031ef-c290-49ba-abbb-8dbfbb0b2ff3",
   "metadata": {
    "language": "python",
    "name": "update_feature_view"
   },
   "outputs": [],
   "source": [
    "# Disable online feature serving \n",
    "# (save costs for latency-less sensitive latency machine learning inference workflows)\n",
    "from snowflake.ml.feature_store.feature_view import OnlineConfig\n",
    "\n",
    "fs.update_feature_view(\n",
    "    name=\"Loan_Feature_View\",\n",
    "    version=VERSION_NUM,\n",
    "    online_config=OnlineConfig(enable=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa68f77c-c6e1-4d2e-82d4-56801f8313e7",
   "metadata": {
    "collapsed": false,
    "name": "generate_dataset_title"
   },
   "source": [
    "## Dataset Generation from the Feature Store\n",
    "\n",
    "__`Snowflake Datasets`__ are deeply integrated into the Snowflake ML ecosystem, enabling a seamless __end-to-end model development and MLOps experience__ entirely within Snowflake. Datasets can be generated directly from feature store features using the __`FeatureStore.generate_dataset`__ API.\n",
    "\n",
    "The `generate_dataset` API __always materializes__ the result, producing an immutable, file-based snapshot of the data. This design:\n",
    "\n",
    "- Ensures __model reproducibility__\n",
    "\n",
    "- Enables efficient ingestion for __large-scale or distributed training__\n",
    "\n",
    "- Decouples model training from upstream feature refresh cycles\n",
    "\n",
    "Generated Snowflake Datasets can be converted into __Snowpark DataFrames__ and passed to __Snowpark ML modeling APIs__ for training. Trained models can then be logged to the __Snowflake Model Registry__, automatically completing the __ML lineage graph__ that links:\n",
    "\n",
    "- Source data\n",
    "\n",
    "- Feature views\n",
    "\n",
    "- Datasets\n",
    "\n",
    "- Models\n",
    "\n",
    "This end-to-end lineage provides __full traceability and governance across the entire ML lifecycle__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d75e36-b26f-4995-ab60-1e62576612cf",
   "metadata": {
    "language": "python",
    "name": "generate_dataset_from_feature_view"
   },
   "outputs": [],
   "source": [
    "ds = fs.generate_dataset(\n",
    "    name=f\"LOAN_DATASET_EXTENDED_FEATURES_{VERSION_NUM}\",\n",
    "    spine_df=sdf.select(\"LOAN_ID\", \"TIMESTAMP\", \"LOAN_PURPOSE\", \"LOAN_RESPONSE\"), # DataFrame containing entity ID, timestamp, label, and additional columns from source data\n",
    "    features=[loan_fv],\n",
    "    spine_timestamp_col=\"TIMESTAMP\",\n",
    "    spine_label_cols=[\"LOAN_RESPONSE\"]\n",
    ")\n",
    "\n",
    "loan_dataset = ds.read.to_snowpark_dataframe() # get a Snowpark DataFrame\n",
    "loan_dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eb3305-3a37-4ad5-bcef-4777c24d62f3",
   "metadata": {
    "language": "python",
    "name": "show_dataset_schema"
   },
   "outputs": [],
   "source": [
    "loan_dataset.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd89d94-c622-416f-9abc-700058b73782",
   "metadata": {
    "collapsed": false,
    "name": "dataset_preprocessing_title"
   },
   "source": [
    "# Dataset preprocessing for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b294f754-ab42-4c95-9380-3e1ed58fac4d",
   "metadata": {
    "language": "python",
    "name": "prepare_dataset_for_ML"
   },
   "outputs": [],
   "source": [
    "import snowflake.ml.modeling.preprocessing as sfml\n",
    "from snowflake.snowpark.types import StringType\n",
    "\n",
    "# encode categorical columns to numeric columns\n",
    "OHE_COLS = loan_dataset.select([col.name for col in loan_dataset.schema if col.datatype==StringType()]).columns\n",
    "\n",
    "sfml_ohe = sfml.OneHotEncoder(input_cols=OHE_COLS, output_cols=OHE_COLS, drop_input_cols=True)\n",
    "loan_dataset_ohe = sfml_ohe.fit(loan_dataset).transform(loan_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e33648-8a39-4194-9b41-abee9a558029",
   "metadata": {
    "language": "python",
    "name": "dataset_ohe_columns"
   },
   "outputs": [],
   "source": [
    "loan_dataset_ohe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc766300-0684-4895-9b5c-b28402beb86d",
   "metadata": {
    "language": "python",
    "name": "rename_dataset_columns"
   },
   "outputs": [],
   "source": [
    "#Rename columns to avoid double nested quotes and white space chars\n",
    "rename_dict = {}\n",
    "for i in loan_dataset_ohe.columns:\n",
    "    if '\"' in i:\n",
    "        rename_dict[i] = i.replace('\"','').replace(' ', '_')\n",
    "\n",
    "loan_dataset_ohe = loan_dataset_ohe.rename(rename_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342f5b86-b0fd-49bc-8609-26d315f88a4d",
   "metadata": {
    "language": "python",
    "name": "check_ohe"
   },
   "outputs": [],
   "source": [
    "loan_dataset_ohe.select(col(\"LOAN_PURPOSE_REFINANCING\")).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a89cfb-1200-4610-b06a-b1631f681e8e",
   "metadata": {
    "language": "python",
    "name": "check_dataset_ohe"
   },
   "outputs": [],
   "source": [
    "loan_dataset_ohe.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089f8cd7-53c3-4171-92a5-d89e69caf260",
   "metadata": {
    "collapsed": false,
    "name": "model_training_title"
   },
   "source": [
    "# Model Training and Evaluation\n",
    "\n",
    "Train and evaluate a __baseline loan lending prediction model__ directly within a __`Snowflake Notebook`__ running on the __`Snowflake Container Runtime for ML`__.\n",
    "\n",
    "Key aspects:\n",
    "\n",
    "- Model training is executed inside Snowflake, eliminating data movement\n",
    "\n",
    "- Distributed compute resources are leveraged for scalable experimentation\n",
    "\n",
    "- A baseline model establishes a performance reference for further tuning and optimization\n",
    "\n",
    "__Baseline Model__\n",
    "\n",
    "Use __`Snowflake ML XGBClassifier`__, Snowflake’s native implementation of the __scikit-learn–compatible XGBoost classifier__, which integrates seamlessly with Snowpark DataFrames and the broader Snowflake ML ecosystem. This enables:\n",
    "\n",
    "- Familiar scikit-learn-style APIs\n",
    "\n",
    "- Efficient training on Snowflake-managed compute\n",
    "\n",
    "- Native compatibility with Snowflake Model Registry and experiment tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5545dfad-d2af-4f63-9299-7aace27e8eee",
   "metadata": {
    "language": "python",
    "name": "train_test_split"
   },
   "outputs": [],
   "source": [
    "loan_dataset_ohe = loan_dataset_ohe.na.drop()\n",
    "\n",
    "train, test = loan_dataset_ohe.random_split(weights=[0.80, 0.20], seed=0)\n",
    "\n",
    "# Split train data into X, y\n",
    "train_pd = train.to_pandas()\n",
    "X_train_pd, y_train_pd = train_pd.drop([\"TIMESTAMP\", \"LOAN_ID\", \"LOAN_RESPONSE\"],axis=1), train_pd[\"LOAN_RESPONSE\"]\n",
    "\n",
    "test_pd = test.to_pandas()\n",
    "X_test_pd, y_test_pd = test_pd.drop([\"TIMESTAMP\", \"LOAN_ID\", \"LOAN_RESPONSE\"],axis=1), test_pd[\"LOAN_RESPONSE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddb3477-d146-48cb-844c-f8431d6754a9",
   "metadata": {
    "language": "python",
    "name": "check_X_train_pd"
   },
   "outputs": [],
   "source": [
    "X_train_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcbc183-9ff0-4471-9943-19e7826b3f7b",
   "metadata": {
    "language": "python",
    "name": "check_X_test_pd"
   },
   "outputs": [],
   "source": [
    "X_test_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1333c9d5-0081-4ad4-84d6-e1645447ec5a",
   "metadata": {
    "language": "python",
    "name": "check_y_train_pd"
   },
   "outputs": [],
   "source": [
    "y_train_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc50ed36-344b-400b-a643-442863b07924",
   "metadata": {
    "language": "python",
    "name": "train_XGBoost_baseline_model_oss"
   },
   "outputs": [],
   "source": [
    "# just as example of oss xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "xgb_base = xgb.XGBClassifier(\n",
    "    max_depth=50,\n",
    "    n_estimators=3,\n",
    "    learning_rate = 0.75,\n",
    "    booster = 'gbtree')\n",
    "\n",
    "# Train model \n",
    "xgb_base.fit(X_train_pd, y_train_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff656983-811f-439b-8ec2-3e55c946463b",
   "metadata": {
    "language": "python",
    "name": "train_XGBoost_baseline_model_SnowflakeML"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml.modeling.xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "xgb_base = XGBClassifier(\n",
    "    input_cols=X_train_pd.columns,\n",
    "    label_cols=y_train_pd.name,\n",
    "    objective='binary:logistic',\n",
    "    max_depth=50,\n",
    "    n_estimators=3,\n",
    "    learning_rate = 0.75,\n",
    "    booster = 'gbtree')\n",
    "\n",
    "# Train model \n",
    "xgb_base.fit(train_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cb2d95-2a91-4d48-a464-0259f419b01b",
   "metadata": {
    "language": "python",
    "name": "evaluate_base_model_train"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "train_preds_base = xgb_base.predict(X_train_pd)\n",
    "train_auc_base = roc_auc_score(y_train_pd,train_preds_base[\"OUTPUT_LOAN_RESPONSE\"])\n",
    "print(\"Train AUC:\", train_auc_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dd40d3-c0aa-4212-9be3-be3c85420a68",
   "metadata": {
    "collapsed": false,
    "name": "model_registry_title"
   },
   "source": [
    "# Managing and Deploying Models with Snowflake Model Registry\n",
    "\n",
    "After training a model, operationalizing it and running inference in Snowflake begins by __logging the model to the Snowflake Model Registry__.\n",
    "\n",
    "The __`Snowflake Model Registry`__ manages machine learning models as __first-class, schema-level objects__, providing end-to-end lifecycle management from development to production.\n",
    "\n",
    "### Key Capabilities\n",
    "\n",
    "The Snowflake Model Registry enables you to:\n",
    "\n",
    "- __Store and manage models__ with versioning, metrics, and rich metadata\n",
    "\n",
    "- __Serve models and run distributed inference at scale__ using Python, SQL, or REST APIs\n",
    "\n",
    "- __Manage model lifecycle transitions__ across dev, test, and production environments with flexible governance\n",
    "\n",
    "- __Monitor model performance and data drift__ using Snowflake ML Observability\n",
    "\n",
    "- __Secure model access__ using role-based access control (RBAC)\n",
    "\n",
    "Once a model is logged, its methods can be invoked to perform inference directly within a __`Snowflake virtual warehouse`__ or served via __`Snowpark Container Services (SPCS)`__ for CPU- or GPU-based, low-latency inference.\n",
    "\n",
    "### Model Registry Python APIs\n",
    "\n",
    "The primary Python classes used to interact with the Snowflake Model Registry are:\n",
    "\n",
    "`snowflake.ml.registry.Registry` – Manages models within a Snowflake schema\n",
    "\n",
    "`snowflake.ml.model.Model` – Represents a registered model\n",
    "\n",
    "`snowflake.ml.model.ModelVersion` – Represents a specific version of a model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d083f314-03fc-4cb8-a84b-aca4f0c404b4",
   "metadata": {
    "language": "python",
    "name": "create_model_registry"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml.registry import Registry\n",
    "\n",
    "model_name=f\"LOAN_MLOPS_{VERSION_NUM}\"\n",
    "\n",
    "model_registry = Registry(\n",
    "    session=session,\n",
    "    database_name=DB,\n",
    "    schema_name=SCHEMA,\n",
    "    options={\"enable_monitoring\": True}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf49c4f-407b-4fcf-a047-feef1583ddde",
   "metadata": {
    "language": "python",
    "name": "register_baseline_model"
   },
   "outputs": [],
   "source": [
    "# Log a model by calling the registry’s log_model method. \n",
    "# log_model method serializes the model — a Python object — and creates a Snowflake model object from it.\n",
    "\n",
    "base_version_name = 'XGB_BASE'\n",
    "\n",
    "try:\n",
    "    mv_base=model_registry.get_model(model_name).version(base_version_name)\n",
    "    print(\"Found existing model version\")\n",
    "except:\n",
    "    print(\"Logging new model version...\")\n",
    "    mv_base = model_registry.log_model(\n",
    "        model_name=model_name,\n",
    "        model=xgb_base,\n",
    "        version_name=base_version_name,   \n",
    "        sample_input_data=train.drop([\"TIMESTAMP\", \"LOAN_ID\", \"LOAN_RESPONSE\"]).limit(100), # using snowpark df to maintain lineage\n",
    "        comment = \"\"\"ML model for predicting loan approval likelihood.\n",
    "                    This model was trained using XGBoost classifier.\n",
    "                    Hyperparameters used were:\n",
    "                    max_depth=50, n_estimators=3, learning_rate = 0.75, algorithm = gbtree.\n",
    "                    \"\"\",\n",
    "        target_platforms= [\"WAREHOUSE\", \"SNOWPARK_CONTAINER_SERVICES\"],\n",
    "        options= {\"enable_explainability\": True}\n",
    "    )\n",
    "\n",
    "    mv_base.set_metric(metric_name=\"Train_ROC_AUC_Score\", value=train_auc_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac45cd1-79fe-4c5a-94d5-7f7522adf2dc",
   "metadata": {
    "language": "python",
    "name": "registry_show_models"
   },
   "outputs": [],
   "source": [
    "model_registry.show_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ddc7ab-e46a-4d01-ad78-ec46aa237a6f",
   "metadata": {
    "language": "python",
    "name": "registry_show_model_versions"
   },
   "outputs": [],
   "source": [
    "model_registry.get_model(model_name).show_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134e6ea5-97c5-41e3-a21b-1a8b9d5e2bac",
   "metadata": {
    "language": "python",
    "name": "show_model_metrics"
   },
   "outputs": [],
   "source": [
    "print(mv_base.show_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913b3791-f14a-4a1a-b90e-944cc88c37fb",
   "metadata": {
    "language": "python",
    "name": "show_model_functions"
   },
   "outputs": [],
   "source": [
    "mv_base.show_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ef5ec9-22ec-4d43-9606-8cda8f4d2b17",
   "metadata": {
    "language": "python",
    "name": "create_prod_tag"
   },
   "outputs": [],
   "source": [
    "# create tag for PROD model\n",
    "session.sql(\"CREATE OR REPLACE TAG PROD\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d19ec6-f1c7-44b8-a07d-0ada3fc432ab",
   "metadata": {
    "language": "python",
    "name": "apply_prod_tag"
   },
   "outputs": [],
   "source": [
    "# apply prod tag\n",
    "m=model_registry.get_model(model_name)\n",
    "m.comment=\"Loan approval prediction models\" # set model level comment\n",
    "m.set_tag(\"PROD\", base_version_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c77bac-1240-45b8-b4c8-d7032cec5da3",
   "metadata": {
    "language": "python",
    "name": "show_model_tags"
   },
   "outputs": [],
   "source": [
    "m.show_tags()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579057a8-d790-4f4c-9917-034ff3e9e5f1",
   "metadata": {
    "collapsed": false,
    "name": "model_version_run_title"
   },
   "source": [
    "A __`ModelVersion`__ object represents a specific version of a registered model that can be executed. Its `run` method is used to invoke the model for inference, either in a Snowflake virtual warehouse or via a Snowpark Container Service, depending on the deployment configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d88e6f-0375-4b7e-9b6a-7e6ac2d6a6de",
   "metadata": {
    "language": "python",
    "name": "evaluate_base_model_on_test"
   },
   "outputs": [],
   "source": [
    "test_preds=mv_base.run(test, function_name=\"predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdaac18-12d0-4983-be18-fe38f3009c35",
   "metadata": {
    "language": "python",
    "name": "prediction_dataframe_columns"
   },
   "outputs": [],
   "source": [
    "test_preds.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1557ff08-e09e-4454-b9d1-907529632f70",
   "metadata": {
    "language": "python",
    "name": "set_test_metric"
   },
   "outputs": [],
   "source": [
    "test_preds_pd = test_preds.select([\"LOAN_RESPONSE\", \"OUTPUT_LOAN_RESPONSE\"]).to_pandas()\n",
    "test_auc = roc_auc_score(test_preds_pd[\"LOAN_RESPONSE\"], test_preds_pd[\"OUTPUT_LOAN_RESPONSE\"])\n",
    "print(\"Test AUC:\", test_auc)\n",
    "\n",
    "mv_base.set_metric(metric_name=\"Test_ROC_AUC_Score\", value=test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9648a-efc4-47de-b906-c33eff6dde48",
   "metadata": {
    "language": "python",
    "name": "show_metrics"
   },
   "outputs": [],
   "source": [
    "print(mv_base.show_metrics())\n",
    "\n",
    "#{'Train_ROC_AUC_Score': 0.8255075926591446, 'Test_ROC_AUC_Score': 0.5966326378748072} - model is overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aa9054-f605-4e24-b1e2-aef033e02163",
   "metadata": {
    "collapsed": false,
    "name": "hpo_title"
   },
   "source": [
    "# Hyperparameter Optimization (HPO) and Experiment Tracking\n",
    "\n",
    "The __`Snowflake ML Hyperparameter Optimization (HPO) API`__ is a model-agnostic framework that enables efficient, parallelized hyperparameter tuning for machine learning models. `HPO runs` can be executed in a __Snowflake Notebook__ configured with the __Container Runtime on Snowpark Container Services (SPCS)__ and scaled across multiple compute nodes.\n",
    "\n",
    "## Benefits of Snowflake HPO\n",
    "\n",
    "- __Automatic distributed training__: Handles parallelization and resource allocation transparently\n",
    "\n",
    "- __Framework-agnostic__: Works with Snowflake ML APIs or any open-source ML framework\n",
    "\n",
    "- __Flexible search strategies__: Supports Bayesian optimization, random search, and both continuous and discrete sampling\n",
    "\n",
    "- __Seamless Snowflake integration__: Efficient data ingestion via Snowflake Datasets or Snowpark DataFrames, with automatic ML lineage capture\n",
    "\n",
    "## Typical HPO Workflow\n",
    "\n",
    "1. __Ingest data__ for training\n",
    "\n",
    "2. __Define the search algorithm__ to guide hyperparameter optimization\n",
    "\n",
    "3. __Specify hyperparameter sampling strategies__\n",
    "\n",
    "4. __Configure the tuner__ with resources and constraints\n",
    "\n",
    "5. __Run training jobs__ for each hyperparameter combination\n",
    "\n",
    "6. __Collect metrics and hyperparameters__ from each training job\n",
    "\n",
    "7. __Analyze results__ to identify the best-performing configuration\n",
    "\n",
    "## Experiment Tracking with Snowflake ML\n",
    "\n",
    "__`Experiments`__ provide a structured way to organize and evaluate model training results. They allow you to:\n",
    "\n",
    "- Compare outcomes of hyperparameter adjustments, different metrics, or model types\n",
    "\n",
    "- Track runs, each containing metadata and artifacts such as hyperparameters, metrics, and training logs\n",
    "\n",
    "- Select the __best-performing model__ in a reproducible, governed workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874885fb-6927-4bf0-b1a8-747133704ef8",
   "metadata": {
    "language": "python",
    "name": "initialize_hpo"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml.data import DataConnector\n",
    "from snowflake.ml.modeling.tune import get_tuner_context\n",
    "from snowflake.ml.modeling import tune\n",
    "from entities import search_algorithm\n",
    "import psutil\n",
    "from snowflake.ml.experiment.experiment_tracking import ExperimentTracking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e2efe2-a7ea-4af0-a3ec-ac1d8cb00155",
   "metadata": {
    "collapsed": false,
    "name": "hpo_data_ingestion_title"
   },
   "source": [
    "## HPO Data Ingestion\n",
    "\n",
    "Before running hyperparameter optimization, the train and test datasets are ingested into the HPO workflow. This is done using the __`dataset_map`__ object, which is a dictionary mapping each dataset (full, features, labels) to its corresponding Snowflake __`DataConnector`__ object, enabling seamless integration with Snowflake ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c413e61c-abf6-48e8-a978-addfc2066944",
   "metadata": {
    "language": "python",
    "name": "create_dataset_map"
   },
   "outputs": [],
   "source": [
    "# Define dataset map\n",
    "X_train = train.drop(\"LOAN_RESPONSE\", \"TIMESTAMP\", \"LOAN_ID\")\n",
    "y_train = train.select(\"LOAN_RESPONSE\")\n",
    "X_test = test.drop(\"LOAN_RESPONSE\",\"TIMESTAMP\", \"LOAN_ID\")\n",
    "y_test = test.select(\"LOAN_RESPONSE\")\n",
    "\n",
    "dataset_map = {\n",
    "    \"train\": DataConnector.from_dataframe(train), \n",
    "    \"X_train\": DataConnector.from_dataframe(X_train), \n",
    "    \"y_train\": DataConnector.from_dataframe(y_train),\n",
    "    \"test\": DataConnector.from_dataframe(test),\n",
    "    \"X_test\": DataConnector.from_dataframe(X_test), \n",
    "    \"y_test\": DataConnector.from_dataframe(y_test)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448ac0a3-80a8-41aa-bc53-69f652c3eb89",
   "metadata": {
    "collapsed": false,
    "name": "cell1"
   },
   "source": [
    "The __`dataset_map`__ is then passed to the __`HPO train function`__, giving each trial access to the required datasets for __training, validation, inference, and metric computation__. This approach ensures that data is __organized, reproducible, and fully compatible__ with the Snowflake ML HPO workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafc2b61-5c3b-4504-ad67-d522117dae2b",
   "metadata": {
    "collapsed": false,
    "name": "hpo_train_function_title"
   },
   "source": [
    "## HPO Train Function\n",
    "\n",
    "The __`HPO train function`__ orchestrates a single trial of the hyperparameter optimization workflow. It leverages the __`TunerContext`__ object to access:\n",
    "\n",
    "- `Hyperparameters` for the current trial\n",
    "\n",
    "- `Dataset mapping` (`dataset_map`) for training, validation, and testing\n",
    "\n",
    "### Workflow Details:\n",
    "\n",
    "1. __Dataset Preparation__:\n",
    "\n",
    "The dataset map separates training and test splits into features and labels, wrapped as DataConnector objects for seamless Snowflake ML integration. These datasets are converted to Pandas DataFrames for model training and evaluation.\n",
    "\n",
    "2. __Experiment Tracking__:\n",
    "\n",
    "Each trial is logged using __`ExperimentTracking`__, capturing hyperparameters, metrics, and artifacts.\n",
    "\n",
    "3. __Model Instantiation and Training__:\n",
    "\n",
    "    - Hyperparameters from `TunerContext` are applied to an `XGBClassifier`\n",
    "\n",
    "    - The model is trained on the full training dataset\n",
    "\n",
    "4. __Inference and Metrics Computation__:\n",
    "\n",
    "    - Predictions are generated on both training and test sets\n",
    "\n",
    "    - Metrics such as __ROC AUC__ are computed for evaluation\n",
    "\n",
    "5. __Logging and Reporting__:\n",
    "\n",
    "    - Hyperparameters and metrics are logged to __`Snowflake ML Experiment Tracking`__\n",
    "\n",
    "    - Metrics and the trained model are reported back to __TunerContext__, enabling automated selection of the best-performing trial\n",
    "\n",
    "This design ensures that each HPO trial is __reproducible, fully tracked, and seamlessly integrated__ into the Snowflake ML workflow, from dataset ingestion through model training, evaluation, and experiment logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a69bde-7539-467c-90c9-177b7f3853c4",
   "metadata": {
    "language": "python",
    "name": "define_training_function"
   },
   "outputs": [],
   "source": [
    "def train_func():\n",
    "\n",
    "    local_session = get_active_session()\n",
    "    exp = ExperimentTracking(session=local_session)\n",
    "    \n",
    "    exp.set_experiment(\"MLOPS_HPO_Experiments\")\n",
    "    \n",
    "    with exp.start_run():\n",
    "        # A context object provided by HPO API to expose data for the current HPO trial        \n",
    "        tuner_context = get_tuner_context()\n",
    "        \n",
    "        # Generate params\n",
    "        config = tuner_context.get_hyper_params()        \n",
    "        dm = tuner_context.get_dataset_map()\n",
    "    \n",
    "        # Log params to experiment tracking\n",
    "        exp.log_params(config)\n",
    "\n",
    "        train_dm_pd = dm[\"train\"].to_pandas().sort_index()\n",
    "        X_train_dm_pd = dm[\"X_train\"].to_pandas().sort_index()\n",
    "        y_train_dm_pd = dm[\"y_train\"].to_pandas().sort_index()        \n",
    "        test_dm_pd = dm[\"test\"].to_pandas().sort_index()\n",
    "        X_test_dm_pd = dm[\"X_test\"].to_pandas().sort_index()\n",
    "        y_test_dm_pd = dm[\"y_test\"].to_pandas().sort_index()\n",
    "       \n",
    "        \n",
    "        # Instantiate model with generated params\n",
    "        model = XGBClassifier(input_cols=X_train_dm_pd.columns,\n",
    "                              label_cols=y_train_dm_pd.columns,\n",
    "                              **config, random_state=42)\n",
    "            \n",
    "    \n",
    "        # Train model, get predictions\n",
    "        model.fit(train_dm_pd)\n",
    "        \n",
    "        # Run inference on train preds\n",
    "        hpo_train_preds = model.predict(train_dm_pd)       \n",
    "\n",
    "        # Run inference on test preds\n",
    "        hpo_test_preds = model.predict(test_dm_pd)\n",
    "           \n",
    "        # Compute metrics \n",
    "        hpo_auc_train = roc_auc_score(hpo_train_preds[\"LOAN_RESPONSE\"], hpo_train_preds[\"OUTPUT_LOAN_RESPONSE\"])\n",
    "\n",
    "        hpo_auc_test = roc_auc_score(hpo_test_preds[\"LOAN_RESPONSE\"], hpo_test_preds[\"OUTPUT_LOAN_RESPONSE\"])\n",
    "                        \n",
    "        metrics_to_log = {\"HPO_ROC_AUC_Train\": hpo_auc_train,\n",
    "                         \"HPO_ROC_AUC_Test\": hpo_auc_test,\n",
    "                         }\n",
    "    \n",
    "        # Log metrics to experiment tracking and tuner context \n",
    "        exp.log_metrics(metrics_to_log)\n",
    "    \n",
    "        tuner_context.report(metrics=metrics_to_log, model=model)\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c65c46-4169-4896-a858-fb77f5f65028",
   "metadata": {
    "collapsed": false,
    "name": "create_tuner_title"
   },
   "source": [
    "## Creating the HPO Tuner\n",
    "\n",
    "The __`Tuner`__ object orchestrates hyperparameter optimization by combining the __training function, search space, and tuner configuration__.\n",
    "\n",
    "__`Search Space`__\n",
    "\n",
    "The search space defines __how hyperparameters are sampled__ during each trial, specifying the range and type of values. Snowflake HPO provides several sampling functions:\n",
    "\n",
    "- `tune.uniform(lower, upper)` – Samples a __continuous value__ uniformly between __lower__ and __upper__. Ideal for parameters like dropout rates or regularization strengths.\n",
    "\n",
    "- `tune.loguniform(lower, upper)` – Samples a value in __logarithmic space__, suitable for parameters spanning several orders of magnitude, e.g., learning rates.\n",
    "\n",
    "- `tune.randint(lower, upper)` – Samples an __integer__ uniformly between __lower__ (inclusive) and __upper__ (exclusive). Useful for discrete parameters like the number of layers.\n",
    "\n",
    "__`Tuner Configuration`__\n",
    "\n",
    "The __`TunerConfig`__ object defines how the HPO process runs, including:\n",
    "\n",
    "- __Metric__: The performance metric to optimize (e.g., __roc_auc_score__, __precision__, __recall__, __F1__, or loss)\n",
    "\n",
    "- __Mode__: Whether the metric should be __maximized__ or __minimized__ (\"max\" or \"min\")\n",
    "\n",
    "- __Search Algorithm__: The strategy for exploring the hyperparameter space. Options include:\n",
    "\n",
    "    - __RandomSearch__ – Randomly samples hyperparameters\n",
    "\n",
    "    - __GridSearch__ – Evaluates every possible combination in a defined grid\n",
    "\n",
    "    - __BayesOpt__ – Uses Bayesian optimization to intelligently select hyperparameters based on previous trial results\n",
    "\n",
    "- __Number of Trials__: Total number of hyperparameter configurations to evaluate\n",
    "\n",
    "- __Concurrency__: Number of trials that can run simultaneously\n",
    "\n",
    "Once the search space and configuration are defined, the `Tuner` object can execute the HPO workflow, running trials in parallel, evaluating metrics, and reporting results back to `TunerContext` and `Experiment Tracking`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ca614e-9ef8-4e6e-b60f-331391493eff",
   "metadata": {
    "language": "python",
    "name": "define_search_space"
   },
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"max_depth\": [10, 20],\n",
    "    \"learning_rate\": [0.05, 0.1],\n",
    "    \"n_estimators\": [500, 1000],    \n",
    "       \n",
    "    \"reg_lambda\": [0.5], # L2\n",
    "    \"scale_pos_weight\": [0.5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb16586-aab3-4837-8c5d-b84bfea8f1dd",
   "metadata": {
    "language": "python",
    "name": "create_tuner"
   },
   "outputs": [],
   "source": [
    "tuner = tune.Tuner(\n",
    "    train_func=train_func,\n",
    "    search_space=search_space,\n",
    "    tuner_config=tune.TunerConfig(\n",
    "        metric=\"HPO_ROC_AUC_Test\",\n",
    "        mode=\"max\",\n",
    "        search_alg=search_algorithm.GridSearch(),\n",
    "        num_trials=8, #run 8 trial runs\n",
    "        max_concurrent_trials=psutil.cpu_count(logical=False) # Use all available CPUs to run distributed HPO. Can also use GPUs here. \n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b744df-0474-4191-9b6a-b7e604e40fc8",
   "metadata": {
    "language": "python",
    "name": "run_hpo"
   },
   "outputs": [],
   "source": [
    "# train several model candidates \n",
    "tuner_results = tuner.run(dataset_map=dataset_map)\n",
    "tuner_results.results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf25dcc-e8d8-4db9-8963-753875646602",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "visualize_HPO_job_progress"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml.runtime_cluster import get_ray_dashboard_url\n",
    "import streamlit as st\n",
    "\n",
    "st.write('View HPO job progress on Ray-powered distributed compute cluster running it by clicking the link below!')\n",
    "st.write('https://'+ get_ray_dashboard_url())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35282cd2-8382-4d80-af5d-f224ca061c06",
   "metadata": {
    "language": "python",
    "name": "select_best_model"
   },
   "outputs": [],
   "source": [
    "tuned_model = tuner_results.best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8cf2fe-a2fa-4a6d-84a4-7f7f4429a99b",
   "metadata": {
    "language": "python",
    "name": "evaluate_tuned_model"
   },
   "outputs": [],
   "source": [
    "opt_preds_train = tuned_model.predict(X_train_pd)\n",
    "\n",
    "train_auc_opt = roc_auc_score(y_train_pd,opt_preds_train[\"OUTPUT_LOAN_RESPONSE\"])\n",
    "print(\"Train AUC OPT:\", train_auc_opt)\n",
    "\n",
    "opt_preds_test = tuned_model.predict(X_test_pd)\n",
    "\n",
    "test_auc_opt = roc_auc_score(y_test_pd,opt_preds_test[\"OUTPUT_LOAN_RESPONSE\"])\n",
    "print(\"Test AUC OPT:\", test_auc_opt)\n",
    "\n",
    "# Train AUC OPT: 0.743872725725519  \n",
    "# Test AUC OPT: 0.6448419192348257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaa5de8-202a-46a4-b1d2-a62619bf17db",
   "metadata": {
    "language": "python",
    "name": "register_optimized_model"
   },
   "outputs": [],
   "source": [
    "# Log the optimized model to the model registry\n",
    "optimized_version_name = 'XGB_Optimized'\n",
    "\n",
    "try:\n",
    "    mv_opt = model_registry.get_model(model_name).version(optimized_version_name)\n",
    "    print(\"Found existing model version!\")\n",
    "except:\n",
    "    print(\"Logging new model version...\")\n",
    "    mv_opt = model_registry.log_model(\n",
    "        model_name=model_name,\n",
    "        model=tuned_model, \n",
    "        version_name=optimized_version_name,\n",
    "        sample_input_data=train.drop([\"TIMESTAMP\", \"LOAN_ID\", \"LOAN_RESPONSE\"]).limit(100),\n",
    "        comment = \"snowflake ml model built of feature store using HPO model\",\n",
    "        target_platforms= [\"WAREHOUSE\", \"SNOWPARK_CONTAINER_SERVICES\"],\n",
    "        options= {\"enable_explainability\": True}\n",
    "    )\n",
    "\n",
    "    mv_opt.set_metric(metric_name=\"Train_ROC_AUC_Score\", value=train_auc_opt)\n",
    "    mv_opt.set_metric(metric_name=\"Test_ROC_AUC_Score\", value=test_auc_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c7d140-2cec-43a4-94ff-be7eb6ac2730",
   "metadata": {
    "language": "python",
    "name": "update_default_model_tag"
   },
   "outputs": [],
   "source": [
    "# Set the optimized model to be the default model version\n",
    "model_registry.get_model(model_name).default = optimized_version_name\n",
    "\n",
    "# Update the PROD tagged model to be the optimized model version\n",
    "m.unset_tag(\"PROD\")\n",
    "m.set_tag(\"PROD\", optimized_version_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e53f6bc-5275-46a2-b8ac-1b69ae81386a",
   "metadata": {
    "collapsed": false,
    "name": "model_explainability_title"
   },
   "source": [
    "# Model Explainability\n",
    "\n",
    "During training, machine learning models learn relationships between inputs and outputs without these relationships being explicitly stated. When a model underperforms, understanding __why__ it behaves a certain way can be challenging. In __regulated industries__ like __finance__ or __healthcare__, explainability is critical to demonstrate that models are producing correct results for the right reasons.\n",
    "\n",
    "## Shapley Values in Snowflake Model Registry\n",
    "\n",
    "Snowflake Model Registry provides __model explainability__ using __Shapley values__, a method to attribute a model’s output to its input features. Key aspects:\n",
    "\n",
    "- __Fair feature attribution__: Shapley values consider all possible combinations of input features to measure the average marginal contribution of each feature to a prediction.\n",
    "\n",
    "- __Insight for interpretability__: Helps debug models and understand why predictions differ across inputs.\n",
    "\n",
    "- __Deviation from background data__: Shapley values report how predictions deviate from an average baseline, calculated using a representative sample of the dataset.\n",
    "\n",
    "## Background Data\n",
    "\n",
    "- The background data provides the baseline for calculating Shapley values. Consistency is critical when comparing Shapley values across datasets.\n",
    "\n",
    "- Some tree-based models may encode background data internally, but most models benefit from explicit background data for accurate explanations.\n",
    "\n",
    "- Snowflake allows up to __1,000 rows__ of background data to be provided when logging a model via the __`sample_input_data`__ parameter or explicitly during analysis.\n",
    "\n",
    "## How It Works\n",
    "\n",
    "- Input features are systematically perturbed and replaced with background data to compute the contribution of each feature.\n",
    "\n",
    "- Positive or negative Shapley values indicate whether a feature __increases__ or __decreases__ the predicted outcome relative to the baseline.\n",
    "\n",
    "By integrating Shapley-based explainability, Snowflake ensures that models are transparent, interpretable, and trustworthy, supporting both debugging and regulatory compliance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33c6220-7444-4480-9ea2-3c64af403353",
   "metadata": {
    "language": "python",
    "name": "generate_shap_values"
   },
   "outputs": [],
   "source": [
    "# Create a sample of records for explanation\n",
    "test_pd_sample=test_pd.rename(columns=rename_dict).sample(n=2500, random_state = 100).reset_index(drop=True)\n",
    "\n",
    "# Compute and retrieve shapley values for each model\n",
    "base_shap_pd = mv_base.run(test_pd_sample, function_name=\"explain\")\n",
    "opt_shap_pd = mv_opt.run(test_pd_sample, function_name=\"explain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af04f5c-c1f3-47bd-a552-148214118c73",
   "metadata": {
    "language": "python",
    "name": "model_explanation_columns"
   },
   "outputs": [],
   "source": [
    "base_shap_pd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22757af5-0a6d-46d4-a64d-3f6543da67c7",
   "metadata": {
    "language": "python",
    "name": "builtin_SF_explainability"
   },
   "outputs": [],
   "source": [
    "# built-in Snowflake's Python visualization functions used to interpret the explainability values of a model\n",
    "from snowflake.ml.monitoring import explain_visualize\n",
    "\n",
    "feat_df=test_pd_sample.drop([\"LOAN_RESPONSE\",\"TIMESTAMP\", \"LOAN_ID\"],axis=1)\n",
    "\n",
    "explain_visualize.plot_influence_sensitivity(base_shap_pd, feat_df, figsize=(1500, 500))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4b4770-4d2c-4977-9d96-d9ee35618dcf",
   "metadata": {
    "language": "python",
    "name": "visualize_feature_importance_shap_OSS"
   },
   "outputs": [],
   "source": [
    "# shap visualizations\n",
    "import shap \n",
    "import numpy as np\n",
    "\n",
    "# Summary plot for base model\n",
    "shap.summary_plot(np.array(base_shap_pd.astype(float)), \n",
    "                  test_pd_sample.drop([\"LOAN_ID\",\"LOAN_RESPONSE\", \"TIMESTAMP\"], axis=1), \n",
    "                  feature_names = test_pd_sample.drop([\"LOAN_ID\",\"LOAN_RESPONSE\", \"TIMESTAMP\"], axis=1).columns)\n",
    "\n",
    "# Summary plot for optimized model\n",
    "shap.summary_plot(np.array(opt_shap_pd.astype(float)), \n",
    "                  test_pd_sample.drop([\"LOAN_ID\",\"LOAN_RESPONSE\", \"TIMESTAMP\"], axis=1), \n",
    "                  feature_names = test_pd_sample.drop([\"LOAN_ID\",\"LOAN_RESPONSE\", \"TIMESTAMP\"], axis=1).columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a83e5ee-18d3-407a-a42a-43586f9fb118",
   "metadata": {
    "language": "python",
    "name": "analyze_feature_impact"
   },
   "outputs": [],
   "source": [
    "# Merge shap vals and actual vals together for easier plotting\n",
    "all_shap_base = test_pd_sample.merge(base_shap_pd, right_index=True, left_index=True, how='outer')\n",
    "all_shap_opt = test_pd_sample.merge(opt_shap_pd, right_index=True, left_index=True, how='outer')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83bf095-641e-4959-990f-fd7a11ac30ad",
   "metadata": {
    "language": "python",
    "name": "rename_expain_columns"
   },
   "outputs": [],
   "source": [
    "all_shap_base = all_shap_base.rename(\n",
    "    columns=lambda c: c.strip('\"')\n",
    ")\n",
    "\n",
    "all_shap_opt = all_shap_opt.rename(\n",
    "    columns=lambda c: c.strip('\"')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5258df15-a809-42b1-95b9-2d1eb0583e8d",
   "metadata": {
    "language": "python",
    "name": "INCOME_feature_importance"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Filter data down to strip outliers\n",
    "asb_filtered = all_shap_base[(all_shap_base.INCOME>0) & (all_shap_base.INCOME<250000)]\n",
    "aso_filtered = all_shap_opt[(all_shap_opt.INCOME>0) & (all_shap_opt.INCOME<250000)]\n",
    "\n",
    "# Set up the figure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
    "fig.suptitle(\"INCOME EXPLANATION\")\n",
    "# Plot side-by-side boxplots\n",
    "sns.scatterplot(data = asb_filtered, x ='INCOME', y = 'INCOME_explanation', ax=axes[0])\n",
    "sns.regplot(data = asb_filtered, x =\"INCOME\", y = 'INCOME_explanation', scatter=False, color='red', line_kws={\"lw\":2},ci =100, lowess=False, ax =axes[0])\n",
    "\n",
    "axes[0].set_title('Base Model')\n",
    "sns.scatterplot(data = aso_filtered, x ='INCOME', y = 'INCOME_explanation',color = \"orange\", ax = axes[1])\n",
    "sns.regplot(data = aso_filtered, x =\"INCOME\", y = 'INCOME_explanation', scatter=False, color='blue', line_kws={\"lw\":2},ci =100, lowess=False, ax =axes[1])\n",
    "axes[1].set_title('Opt Model')\n",
    "\n",
    "# Customize and show the plot\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"Income\")\n",
    "    ax.set_ylabel(\"Influence\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7421a5-699b-414a-8b60-2bba998ae141",
   "metadata": {
    "language": "python",
    "name": "LOAN_AMOUNT_feature_importance"
   },
   "outputs": [],
   "source": [
    "# Filter data down to strip outliers\n",
    "asb_filtered = all_shap_base[all_shap_base.LOAN_AMOUNT<2000000]\n",
    "aso_filtered = all_shap_opt[all_shap_opt.LOAN_AMOUNT<2000000]\n",
    "\n",
    "\n",
    "# Set up the figure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
    "fig.suptitle(\"LOAN_AMOUNT EXPLANATION\")\n",
    "# Plot side-by-side boxplots\n",
    "sns.scatterplot(data = asb_filtered, x ='LOAN_AMOUNT', y = 'LOAN_AMOUNT_explanation', ax=axes[0])\n",
    "sns.regplot(data = asb_filtered, x =\"LOAN_AMOUNT\", y = 'LOAN_AMOUNT_explanation', scatter=False, color='red', line_kws={\"lw\":2},ci =100, lowess=True, ax =axes[0])\n",
    "axes[0].set_title('Base Model')\n",
    "\n",
    "sns.scatterplot(data = aso_filtered, x ='LOAN_AMOUNT', y = 'LOAN_AMOUNT_explanation',color = \"orange\", ax = axes[1])\n",
    "sns.regplot(data = aso_filtered, x =\"LOAN_AMOUNT\", y = 'LOAN_AMOUNT_explanation', scatter=False, color='blue', line_kws={\"lw\":2},ci =100, lowess=True, ax =axes[1])\n",
    "axes[1].set_title('Opt Model')\n",
    "\n",
    "# Customize and show the plot\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"LOAN_AMOUNT\")\n",
    "    ax.set_ylabel(\"Influence\")\n",
    "    # ax.set_xlim((0,10000))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c93940-cf36-4c9c-8966-a5d8394a6922",
   "metadata": {
    "language": "python",
    "name": "LOAN_PURPOSE_HOME_PURCHASE_feature_importance"
   },
   "outputs": [],
   "source": [
    "# Set up the figure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
    "fig.suptitle(\"HOME PURCHASE LOAN EXPLANATION\")\n",
    "# Plot side-by-side boxplots\n",
    "sns.boxplot(data = all_shap_base, x ='LOAN_PURPOSE_HOME_PURCHASE', y = 'LOAN_PURPOSE_HOME_PURCHASE_explanation',\n",
    "            hue='LOAN_PURPOSE_HOME_PURCHASE', width=0.8, ax=axes[0])\n",
    "axes[0].set_title('Base Model')\n",
    "sns.boxplot(data = all_shap_opt, x ='LOAN_PURPOSE_HOME_PURCHASE', y = 'LOAN_PURPOSE_HOME_PURCHASE_explanation',\n",
    "            hue='LOAN_PURPOSE_HOME_PURCHASE', width=0.4, ax = axes[1])\n",
    "axes[1].set_title('Opt Model')\n",
    "\n",
    "# Customize and show the plot\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"Home PURCHASE Loan (1 = True)\")\n",
    "    ax.set_ylabel(\"Influence\")\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a7a800-a286-4cf1-b51c-9d3d36a7fb17",
   "metadata": {
    "collapsed": false,
    "name": "model_inference_on_warehouse_title"
   },
   "source": [
    "# Model Inference on Warehouse using Registered Model and Snowflake Python Stored Procedure \n",
    "\n",
    "Models registered in the __Snowflake Model Registry__ can be executed directly within a __Snowflake virtual warehouse__ (default). This method is ideal for small-to-medium CPU-only models, whose dependencies are available via the Snowflake Conda channel. The method requires no container setup and is best for testing or moderate-sized datasets.\n",
    "\n",
    "For __warehouse-based inference__, the `ModelVersion` object (mv) is retrieved from the Model Registry and its `run` method is invoked to make predictions with following parameters:\n",
    "- `X` - the input inference data provided as a __Snowpark__ or __pandas DataFrame__.\n",
    "- `function_name` - the function of the model to call (e.g., `predict`)\n",
    "\n",
    "`run` returns a DataFrame of the same type as the input, containing model predictions.\n",
    "\n",
    "For __automated, table-based batch inference__, model predictions obtained via `mv.run` are wrapped in a __Snowflake Python stored procedure__, enabling inference on entire tables and saving results back to Snowflake.\n",
    "\n",
    "__Key features__:\n",
    "- Automates __batch inference__ on production-scale tables.\n",
    "- Uses the __registered model version__ from the Snowflake Model Registry.\n",
    "- Predictions are joined back to the input table and saved in Snowflake.\n",
    "- Supports __schema evolution__ and reusable calls from SQL or Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0772b563-2577-457a-9b02-594d1b70c23b",
   "metadata": {
    "language": "python",
    "name": "persist_data_for_inference_and_model_logging"
   },
   "outputs": [],
   "source": [
    "train.write.save_as_table(f\"LOANS_TRAIN_{VERSION_NUM}\", mode=\"overwrite\")\n",
    "test.write.save_as_table(f\"LOANS_TEST_{VERSION_NUM}\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8379ef-7b4f-4762-ad76-f8cf3aa4b48f",
   "metadata": {
    "language": "sql",
    "name": "cell6"
   },
   "outputs": [],
   "source": [
    "SHOW TABLES;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac6a6c9-9e82-4297-ad68-55f1656ae30b",
   "metadata": {
    "language": "python",
    "name": "create_inference_stored_procedure"
   },
   "outputs": [],
   "source": [
    "from snowflake import snowpark\n",
    "from snowflake.snowpark.types import StringType\n",
    "from snowflake.ml.registry import Registry\n",
    "\n",
    "\n",
    "def inference_sproc(session: snowpark.Session, table_name: str, modelname: str, modelversion: str) -> str:\n",
    "\n",
    "    reg = Registry(session=session)\n",
    "    m = reg.get_model(modelname)  # Fetch the model using the registry\n",
    "    mv = m.version(modelversion)\n",
    "    \n",
    "    input_table_name=table_name\n",
    "    pred_col = f'{modelversion}_PREDICTION'\n",
    "\n",
    "    # Read the input table to a dataframe\n",
    "    sdf = session.table(input_table_name)\n",
    "    results = mv.run(sdf, function_name=\"predict\") \\\n",
    "        .select(\"LOAN_ID\", \"OUTPUT_LOAN_RESPONSE\") \\\n",
    "        .withColumnRenamed(\"OUTPUT_LOAN_RESPONSE\", pred_col)\n",
    "    # 'results' is the output DataFrame with predictions\n",
    "\n",
    "    final = sdf.join(results, on=\"LOAN_ID\", how=\"full\")\n",
    "    \n",
    "    # Write results back to Snowflake table\n",
    "    final.write.save_as_table(table_name, mode='overwrite', enable_schema_evolution=True)\n",
    "\n",
    "    return \"Success\"\n",
    "\n",
    "# Register the stored procedure\n",
    "session.sproc.register(\n",
    "    func=inference_sproc,\n",
    "    name=\"inference_sproc\",\n",
    "    replace=True,\n",
    "    is_permanent=True,\n",
    "    stage_location=\"@MLOPS_STAGE\",\n",
    "    packages=['joblib', 'snowflake-snowpark-python', 'snowflake-ml-python'],\n",
    "    return_type=StringType()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac4c827-e6a9-42db-920a-9b184f207ae3",
   "metadata": {
    "language": "sql",
    "name": "call_inference_sproc_train_base"
   },
   "outputs": [],
   "source": [
    "CALL inference_sproc('LOANS_TRAIN_{{VERSION_NUM}}','{{model_name}}', '{{base_version_name}}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f2b6ed-5ac9-48f8-84d3-4d4202ce6d0b",
   "metadata": {
    "language": "sql",
    "name": "call_inference_sproc_test_base"
   },
   "outputs": [],
   "source": [
    "CALL inference_sproc('LOANS_TEST_{{VERSION_NUM}}','{{model_name}}', '{{base_version_name}}');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b90f1aa-e553-4bd7-b7a4-bad42d65be8d",
   "metadata": {
    "language": "sql",
    "name": "call_inference_sproc_train_opt"
   },
   "outputs": [],
   "source": [
    "CALL inference_sproc('LOANS_TRAIN_{{VERSION_NUM}}','{{model_name}}', '{{optimized_version_name}}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c2c3e6-c6bc-49ba-b678-c544ec0b6a29",
   "metadata": {
    "language": "sql",
    "name": "call_inference_sproc_test_opt"
   },
   "outputs": [],
   "source": [
    "CALL inference_sproc('LOANS_TEST_{{VERSION_NUM}}','{{model_name}}', '{{optimized_version_name}}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7276c5a5-c91d-4e02-b6d3-d52de393bded",
   "metadata": {
    "language": "sql",
    "name": "retrieve_predictions"
   },
   "outputs": [],
   "source": [
    "SELECT TIMESTAMP, LOAN_ID, INCOME, LOAN_AMOUNT, XGB_BASE_PREDICTION, XGB_OPTIMIZED_PREDICTION, LOAN_RESPONSE \n",
    "FROM LOANS_TEST_{{VERSION_NUM}} \n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f0acbf-ce90-46ee-99e2-8e202f524a23",
   "metadata": {
    "collapsed": false,
    "name": "ml_observability_title"
   },
   "source": [
    "# ML Observability:  Monitoring model behavior over time\n",
    "\n",
    "`ML Observability` in Snowflake enables traking the behavior and quality of production models deployed via Snowflake Model Registry. By monitoring models over time, can detect issues early, understand performance trends, and ensure reliable predictions.\n",
    "\n",
    "## Key capabilities:\n",
    "- __Performance Monitoring__: Track metrics such as acuuracy, ROC AUC, precision, recall, or custom metrics over time using stored inference data.\n",
    "- __Drift Detection__: Detect changes in input feature distributions or target labels to identify __data drift__ or __concept drift__, which can impact model performance.\n",
    "- __Volume Monitoring__: Observe the number of predictions over time to detect operational anomalies or changes in usage patterns,\n",
    "- __Segmented Monitoring__: Analyze model performance across different __data segments__, such as categories in string columns, to ensure consistent behavior for all groups.\n",
    "\n",
    "## How It Works:\n",
    "- ML Observability uses __stored inference data__ from deployed models.\n",
    "- Metrics and statistics can be automatically calculated and visualized over time,\n",
    "- Alerts or dashboards can be set up to flag deviations or unexpected patterns.\n",
    "\n",
    "By leveraging ML Observability, organizations gain __continuous insight__ into model health, enabling proactive maintenance, timely retraining, and compliance with regulatory standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158c14cd-12d4-4644-9b61-c18ea8b7abf0",
   "metadata": {
    "language": "sql",
    "name": "prepare_for_segmentation"
   },
   "outputs": [],
   "source": [
    "-- create segments\n",
    "\n",
    "ALTER TABLE LOANS_TRAIN_{{VERSION_NUM}}\n",
    "ADD COLUMN IF NOT EXISTS LOAN_PURPOSE VARCHAR(50);\n",
    "\n",
    "UPDATE LOANS_TRAIN_{{VERSION_NUM}}\n",
    "SET LOAN_PURPOSE = CASE\n",
    "    WHEN LOAN_PURPOSE_HOME_IMPROVEMENT = 1 THEN 'HOME_IMPROVEMENT'\n",
    "    WHEN LOAN_PURPOSE_HOME_PURCHASE = 1 THEN 'HOME_PURCHASE'\n",
    "    WHEN LOAN_PURPOSE_REFINANCING = 1 THEN 'REFINANCING'\n",
    "    ELSE 'OTHER'\n",
    "END;\n",
    "\n",
    "ALTER TABLE LOANS_TEST_{{VERSION_NUM}}\n",
    "ADD COLUMN IF NOT EXISTS LOAN_PURPOSE VARCHAR(50);\n",
    "\n",
    "UPDATE LOANS_TEST_{{VERSION_NUM}}\n",
    "SET LOAN_PURPOSE = CASE\n",
    "    WHEN LOAN_PURPOSE_HOME_IMPROVEMENT = 1 THEN 'HOME_IMPROVEMENT'\n",
    "    WHEN LOAN_PURPOSE_HOME_PURCHASE = 1 THEN 'HOME_PURCHASE'\n",
    "    WHEN LOAN_PURPOSE_REFINANCING = 1 THEN 'REFINANCING'\n",
    "    ELSE 'OTHER'\n",
    "END;\n",
    "\n",
    "SELECT LOAN_PURPOSE_HOME_PURCHASE, LOAN_PURPOSE_HOME_IMPROVEMENT, LOAN_PURPOSE_REFINANCING, LOAN_PURPOSE FROM LOANS_TEST_{{VERSION_NUM}} limit 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c23ff00-5b4a-453d-9e59-70ec92702a7c",
   "metadata": {
    "collapsed": false,
    "name": "model_monitor_title"
   },
   "source": [
    "# Model Monitor\n",
    "\n",
    "A __`Model Monitor`__ in Snowflake is a __first-class object__ created for each model version that requires ongoing monitoring. It enables __continuous tracking of model behavior and performance__ using __stored inference data__.\n",
    "\n",
    "## Key Attributes of a Model Monitor \n",
    "\n",
    "Each monitor encapsulates configuration and metadata required to track a specific model version:\n",
    "\n",
    "- `Model & Model Version`: the model and its specific version to monitor.\n",
    "\n",
    "- `Function Name`: the specific method in the model version to monitor (e.g., predict).\n",
    "\n",
    "- `Source table`: the Snowflake table where monitor logs are stored.\n",
    "\n",
    "- `Warehouse`: The Snowflake warehouse used for the monitor’s internal compute operations.\n",
    "\n",
    "- `Refresh interval`: How frequently the monitor updates its internal state (minimum: 60 seconds). \n",
    "\n",
    "- `Aggregation Window`: The minimum time granularity for storing monitoring data (minimum: 1 day).\n",
    "\n",
    "- `Timestamp Column`: The column of type TIMESTAMP_NTZ in the source data containing timestamps.\n",
    "\n",
    "- `Baseline Table (Optional)`: A snapshot of historical or reference data  used for computing drift and comparative metrics. Required to detect drift.\n",
    "\n",
    "- `ID Columns (Optional)`: Array of column names that uniquely identify each row in the source data.\n",
    "\n",
    "- `Prediction Class Columns (Optional)`: Columns representing model predictions. For binary classification or regression tasks, columns must be of type NUMBER. For multi-class classification, must be STRING.\n",
    "\n",
    "- `Actual Class Columns (Optional)`: Columns containing true target values from the source data.\n",
    "\n",
    "- `Segment Columns (Optional)`: Columns in source data used to segment data for performance monitoring (must be STRING). \n",
    "\n",
    "## How It Works:\n",
    "\n",
    "- The monitor __automatically refreshes its logs__ by querying source data at the specified refresh interval.\n",
    "\n",
    "- Monitoring reports are updated based on the aggregated logs.\n",
    "\n",
    "- Supports tracking __performance metrics, drift, and segment-level insights__.\n",
    "\n",
    "Using a Model Monitor ensures that the deployed models are __continuously observed__, enabling proactive alerts, early detection of drift, and reliable model operations over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af2fe8b-e3ae-4b70-a370-16e9a705ea21",
   "metadata": {
    "language": "sql",
    "name": "create_monitor_base_model"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE MODEL MONITOR LOANS_BASE_MODEL_MONITOR\n",
    "WITH\n",
    "    MODEL={{model_name}}\n",
    "    VERSION={{base_version_name}}\n",
    "    FUNCTION=predict\n",
    "    SOURCE=LOANS_TEST_{{VERSION_NUM}}\n",
    "    BASELINE=LOANS_TRAIN_{{VERSION_NUM}}\n",
    "    TIMESTAMP_COLUMN=TIMESTAMP\n",
    "    PREDICTION_CLASS_COLUMNS=(XGB_BASE_PREDICTION)  \n",
    "    ACTUAL_CLASS_COLUMNS=(LOAN_RESPONSE)\n",
    "    ID_COLUMNS=(LOAN_ID)\n",
    "    SEGMENT_COLUMNS = (LOAN_PURPOSE)\n",
    "    WAREHOUSE={{COMPUTE_WAREHOUSE}}\n",
    "    REFRESH_INTERVAL='12 hours'\n",
    "    AGGREGATION_WINDOW='1 day';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1856e1a1-9737-488c-8acd-0e18490b039c",
   "metadata": {
    "language": "sql",
    "name": "create_monitor_opt_model"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE MODEL MONITOR LOANS_OPTIMIZED_MODEL_MONITOR\n",
    "WITH\n",
    "    MODEL={{model_name}}\n",
    "    VERSION={{optimized_version_name}}\n",
    "    FUNCTION=predict\n",
    "    SOURCE=LOANS_TEST_{{VERSION_NUM}}\n",
    "    BASELINE=LOANS_TRAIN_{{VERSION_NUM}}\n",
    "    TIMESTAMP_COLUMN=TIMESTAMP\n",
    "    PREDICTION_CLASS_COLUMNS=(XGB_OPTIMIZED_PREDICTION)  \n",
    "    ACTUAL_CLASS_COLUMNS=(LOAN_RESPONSE)\n",
    "    ID_COLUMNS=(LOAN_ID)\n",
    "    SEGMENT_COLUMNS = (LOAN_PURPOSE)\n",
    "    WAREHOUSE={{COMPUTE_WAREHOUSE}}\n",
    "    REFRESH_INTERVAL='12 hours'\n",
    "    AGGREGATION_WINDOW='1 day';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac2c5f4-7bc0-43e8-9f86-0fc0bf87eae7",
   "metadata": {
    "language": "python",
    "name": "viewing_monitoring_reports"
   },
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "\n",
    "st.write('Click the link below to visualize monitored model reports!')\n",
    "st.write(f'https://app.snowflake.com/{ORG_NAME}/{ACCOUNT_NAME}/#/data/databases/{DB}/schemas/{SCHEMA}/model/{model_name.upper()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cd8dda-180f-4294-b5d0-fa5d302c82ef",
   "metadata": {
    "collapsed": false,
    "name": "querying_monitor_results"
   },
   "source": [
    "## Querying Model Monitoring Results\n",
    "\n",
    "Each __Model Monitor__ automatically computes and tracks a variety of metrics for the monitored model version. These metrics help understand model behavior, detect drift, and ensure consistent performance over time.\n",
    "\n",
    "### Types of Metrics\n",
    "\n",
    "- __`Drift Metrics`__: Track changes in input feature distributions or target distributions over time. Detects __data drift__ or __concept drift__.\n",
    "\n",
    "- __`Performance Metrics`__: Track changes in model performance, such as ROC AUC, accuracy, or other user-defined metrics.\n",
    "\n",
    "- __`Statistical Metrics`__: Basic statistics on the source data, such as row counts, null values, or distribution summaries.\n",
    "\n",
    "### Querying Metrics\n",
    "\n",
    "Snowflake provides dedicated functions to query metrics for a model monitor:\n",
    "\n",
    "| Metric Type         | Query Function                     | Description                                                                  |\n",
    "| ------------------- | ---------------------------------- | ---------------------------------------------------------------------------- |\n",
    "| Drift Metrics       | `MODEL_MONITOR_DRIFT_METRIC`       | Returns metrics on distribution or statistical shifts in monitored features. |\n",
    "| Performance Metrics | `MODEL_MONITOR_PERFORMANCE_METRIC` | Returns metrics tracking model prediction quality over time.                 |\n",
    "| Statistical Metrics | `MODEL_MONITOR_STAT_METRIC`        | Returns statistics on data quality, counts, or null values.                  |\n",
    "\n",
    "These functions allow to __analyze, visualize, and alert__ on __model behavior trends__ directly within Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983454be-9464-41c4-b298-68ec6e0a4186",
   "metadata": {
    "language": "sql",
    "name": "monitor_drift"
   },
   "outputs": [],
   "source": [
    "SELECT * FROM TABLE(MODEL_MONITOR_DRIFT_METRIC (\n",
    "    'LOANS_BASE_MODEL_MONITOR', -- model monitor to use\n",
    "    'DIFFERENCE_OF_MEANS', -- metric for computing drift\n",
    "    'XGB_BASE_PREDICTION', -- column to compute drift on\n",
    "    '1 DAY',  -- day granularity for drift computation\n",
    "    DATEADD(DAY, -90, CURRENT_DATE()), -- end date\n",
    "    DATEADD(DAY, -60, CURRENT_DATE()) -- start date\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ff8eee-b71c-4d9e-8536-e8b1ce6e1869",
   "metadata": {
    "collapsed": false,
    "name": "model_serving_title"
   },
   "source": [
    "# Model Serving: Running Inference in Snowpark Container Services (SPCS)\n",
    "\n",
    "The __Snowflake Model Registry__ allows models to run either:\n",
    "\n",
    "1. Directly in a __warehouse__ (default, CPU-only, small-to-medium datasets).\n",
    "2. In a `Snowpark Container Services (SPCS) compute pool` via ` Model Serving` for scalable, low-latency inference.\n",
    "\n",
    "## How SPCS Model Serving Works\n",
    "\n",
    "- Snowflake builds an __inference container image__ containing:\n",
    "    - The registered model\n",
    "    - Dependencies, including PyPI packages or other sources\n",
    "\n",
    "- The container image is stored in an __image repository__.\n",
    "    - Snowflake provides a __default image repository__, automatically used if no custom repository is provided.\n",
    "    - To use a __custom repository__, create one and pass its name via the `image_repo` argument in the `create_service` method.\n",
    "    - Multiple users or roles can __share the same container image__ to save time and compute costs.\n",
    "    - Use a __single image repository__ to avoid rebuilding the image multiple times and enable reuse by all users.\n",
    "    - All roles that will use the repository must have the following privileges:\n",
    "        - SERVICE READ & WRITE\n",
    "        - REPO READ & WRITE\n",
    "    - Keep wite privileges even after the image is initially built, as updates to dependencies may require rebuilding the image.    \n",
    "\n",
    "  \n",
    "- The container runs in a Snowflake __compute pool__ and exposes the model for inference to serve predictions. A __compute pool__ is a collection of one or more VM nodes on which Snowflake runs SPCS services. Creating a compute pool involves specifying at minimum:\n",
    "\n",
    "    - __Instance Family__ to provision for the compute pool nodes (e.g., CPU_X64_L)\n",
    "\n",
    "    - __Minimum Nodes__ to launch the compute pool with\n",
    "\n",
    "    - __Maximum Nodes__ the compute pool can scale to; prevents an unexpectedly large number of nodes from being added to the compute pool by Snowflake autoscaling.\n",
    "\n",
    "- __`Ingress HTTP endpoints`__ can be provisioned for external applications to call the model from the public Internet.\n",
    "\n",
    "- __`Each created service for SPCS deployment`__ - has an __internal DNS name__, and a public endpoint is created if `ingress_enabled=True`.\n",
    "\n",
    "## Deploying a Model to SPCS\n",
    "\n",
    "1. Obtain a __registered__ `ModelVersion` object (`mv`).\n",
    "\n",
    "2. Create a service for SPCS deployment using the `create_service` method of `mv`.\n",
    "\n",
    "3. Run inference using the service by calling the `run` method of `mv` including the `service_name` parameter. Omitting `service_name` will default to running inference in a warehouse.\n",
    "\n",
    "## Accessing Service Endpoints\n",
    "\n",
    "- __Public HTTP endpoint (external calls)__:\n",
    "\n",
    "    - Use `SHOW ENDPOINT` or `mv.list_services()`\n",
    "\n",
    "    - The `ingress_url` output column has the format: <unique-service-id>-<account-id>.snowflakecomputing.app\n",
    "\n",
    "    - Call the service via: `https://<unique-service-id>-<account-id>.snowflakecomputing.app/<method-name>` (e.g., `https://ingress_url/predict`)\n",
    "\n",
    "- __Internal DNS (internal calls within Snowflake)__:\n",
    "\n",
    "- Use `DESCRIBE SERVICE` to retrieve the `dns_name` column\n",
    "\n",
    "- Call the service via: `https://<dns_name>:<port>/<method_name>`\n",
    "\n",
    "\n",
    "\n",
    "## Key benefits\n",
    "\n",
    "- __Scalable Inference__: Automatically leverages a distributed compute pool. Enables scalable, low-latency predictions for production workloads.\n",
    "\n",
    "- __Custom Dependencies__: Full control over Python packages and libraries.\n",
    "\n",
    "- __Efficient Sharing__: Single image repository can be resued across users and roles.\n",
    "\n",
    "- __External Accessibility__: Optionally expose models via public HTTP endpoints for external applications.\n",
    "\n",
    "- __Seamless Snowflake Integration__: Works with Snowflake Datasets, Feature Store, and Model Registry for full end-to-end ML lifecycle management.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616144e7-43bd-470c-a9c2-6cee5afa052a",
   "metadata": {
    "language": "sql",
    "name": "alter_compute_pool"
   },
   "outputs": [],
   "source": [
    "-- Increase compute pool's instance type and number of nodes for multi-node inference\n",
    "\n",
    "ALTER COMPUTE POOL MLOPS_COMPUTE_POOL SET MIN_NODES=2;\n",
    "ALTER COMPUTE POOL MLOPS_COMPUTE_POOL SET MAX_NODES=2;\n",
    "ALTER COMPUTE POOL MLOPS_COMPUTE_POOL SET INSTANCE_FAMILY=\"CPU_X64_L\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a5350f-bf56-46eb-8501-e1c5138b60a9",
   "metadata": {
    "language": "python",
    "name": "create_service"
   },
   "outputs": [],
   "source": [
    "model_name=f\"LOAN_MLOPS_{VERSION_NUM}\"\n",
    "mv_opt = model_registry.get_model(model_name).version(\"XGB_OPTIMIZED\")\n",
    "\n",
    "mv_opt.create_service(\n",
    "    service_name=\"MLOPS_LOANS_INFERENCE_SPCS_SERVICE\",\n",
    "    service_compute_pool=\"MLOPS_COMPUTE_POOL\",\n",
    "    ingress_enabled=True,\n",
    "    max_instances=2   # 2 service instances running\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8f975f-a99b-4b25-ac8f-e15a931aad8c",
   "metadata": {
    "language": "python",
    "name": "model_version"
   },
   "outputs": [],
   "source": [
    "model_registry.get_model(model_name).version(\"XGB_OPTIMIZED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d40ddb-357d-45a2-813f-86b1e462db46",
   "metadata": {
    "language": "python",
    "name": "model_version_showing_inference_services"
   },
   "outputs": [],
   "source": [
    "model_registry.get_model(model_name).show_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9565a6c1-46d6-4be6-a7b6-2d4c479028e1",
   "metadata": {
    "language": "python",
    "name": "run_inference_on_spcs_service"
   },
   "outputs": [],
   "source": [
    "mv_container = model_registry.get_model(model_name).default\n",
    "mv_container.run(test, function_name = \"predict\", service_name = \"MLOPS_LOANS_INFERENCE_SPCS_SERVICE\").rename(\"OUTPUT_LOAN_RESPONSE\", 'XGB_PREDICTION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8826c9-3313-4fde-9062-075125291bc8",
   "metadata": {
    "language": "sql",
    "name": "show_endpoints_in_service"
   },
   "outputs": [],
   "source": [
    "SHOW ENDPOINTS IN SERVICE {{DB}}.{{SCHEMA}}.MLOPS_LOANS_INFERENCE_SPCS_SERVICE;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ee334b-f937-4aff-8787-7e3cea8fae2e",
   "metadata": {
    "language": "python",
    "name": "list_services"
   },
   "outputs": [],
   "source": [
    "mv_container.list_services()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbaf98d-f65a-4c33-96b3-ac74b379ef32",
   "metadata": {
    "language": "sql",
    "name": "describe_service"
   },
   "outputs": [],
   "source": [
    "DESCRIBE SERVICE {{DB}}.{{SCHEMA}}.MLOPS_LOANS_INFERENCE_SPCS_SERVICE;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a073247c-ea55-4681-920f-7395868d346a",
   "metadata": {
    "language": "python",
    "name": "call_endpoint"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "def get_headers(pat_token):\n",
    "    headers = {'Authorization': f'Snowflake Token=\"{pat_token}\"'}\n",
    "    return headers\n",
    "\n",
    "headers = get_headers(os.getenv(\"PAT_TOKEN\"))\n",
    "\n",
    "URL = 'https://<ingress_url>/predict'\n",
    "\n",
    "# Prepare data to be sent\n",
    "test_pd = test.to_pandas()\n",
    "data = {\"data\": np.column_stack([range(test_pd.shape[0]), test_pd.values]).tolist()}\n",
    "\n",
    "# Send over HTTP\n",
    "def send_request(data: dict):\n",
    "    output = requests.post(URL, json=data, headers=headers)\n",
    "    assert (output.status_code == 200), f\"Failed to get response from the service. Status code: {output.status_code}\"\n",
    "    return output.content\n",
    "\n",
    "results = send_request(data=data)\n",
    "print(json.loads(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33126ec2-57d5-4217-b854-e697ef1f6056",
   "metadata": {
    "collapsed": false,
    "name": "image_repository_title"
   },
   "source": [
    "When a model is deployed to Snowpark Container Services, Snowflake Model Serving builds a `container inference image` that holds the model and its dependencies. The container image is stored in an `IMAGE REPOSITORY`. Snowflake provides a default image repository, which is used automatically when no custom repository is provided when creating a service. To use a custom repository, create an image repository and pass its name in the `image_repo` argument of the `create_service` method.\n",
    "\n",
    "Sharing the image repository\n",
    "\n",
    "It is common for multiple users or roles to use the same model. Using a single image repository allows the image to be built once and reused by all users, saving time and expense. All roles that will use the repo need the SERVICE READ, SERVICE WRITE, READ, and WRITE privileges on the repo. Since the image might need to be rebuilt to update dependencies, keep the write privileges; don’t revoke them after the image is initially built.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c8529e-18c8-4296-b2f5-3c6bfb6a297b",
   "metadata": {
    "language": "sql",
    "name": "stop_compute_pool"
   },
   "outputs": [],
   "source": [
    "ALTER COMPUTE POOL MLOPS_COMPUTE_POOL STOP ALL;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "lac@mechatronix.ca",
   "authorId": "295420094223",
   "authorName": "SNOWWHITE2026",
   "lastEditTime": 1768788350559,
   "notebookId": "2crn4bczldo7bjyr5xoq",
   "sessionId": "4c4ee659-fbd1-4fff-81d2-44b4e908ff25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
